{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7810104,"sourceType":"datasetVersion","datasetId":4574341}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORT PACKAGES","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport random\nfrom transformers import BertTokenizer, BertModel\nimport torch\nimport string\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport torchtext.vocab as vocab\nfrom torch import nn","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:04:07.264032Z","iopub.execute_input":"2024-03-18T21:04:07.264312Z","iopub.status.idle":"2024-03-18T21:04:16.971751Z","shell.execute_reply.started":"2024-03-18T21:04:07.264286Z","shell.execute_reply":"2024-03-18T21:04:16.970846Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# LA PREMIERE PARTIE: data preparation","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ntrain_data = pd.read_excel('/kaggle/input/tweets-datasetsxlsx/test.xlsx', nrows=200)\ntest_data = pd.read_excel('/kaggle/input/tweets-datasetsxlsx/test.xlsx', nrows=200)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:04:16.973464Z","iopub.execute_input":"2024-03-18T21:04:16.974016Z","iopub.status.idle":"2024-03-18T21:04:17.585656Z","shell.execute_reply.started":"2024-03-18T21:04:16.973983Z","shell.execute_reply":"2024-03-18T21:04:17.584953Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Fonction pour créer des paires de tweets aléatoires\ndef create_random_tweet_pairs(data, num_pairs):\n    tweet_pairs = []\n    tweet_indices = list(range(len(data)))\n    for _ in range(num_pairs):\n        i, j = random.sample(tweet_indices, 2)\n        tweet_pairs.append((data.iloc[i]['text'], data.iloc[j]['text']))\n    return tweet_pairs\n\n\n# Fonction pour étiqueter les paires de tweets en fonction de la similarité des utilisateurs\ndef label_tweet_pairs(tweet_pairs, data):\n    labeled_pairs = []\n    for pair in tweet_pairs:\n        tweet1, tweet2 = pair\n        user1 = data[data['text'] == tweet1]['user'].iloc[0]\n        user2 = data[data['text'] == tweet2]['user'].iloc[0]\n        similarity_label = 1 if user1 == user2 else 0\n        labeled_pairs.append((tweet1, tweet2, user1, user2, similarity_label))\n    return labeled_pairs","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:04:17.586794Z","iopub.execute_input":"2024-03-18T21:04:17.587273Z","iopub.status.idle":"2024-03-18T21:04:17.595588Z","shell.execute_reply.started":"2024-03-18T21:04:17.587244Z","shell.execute_reply":"2024-03-18T21:04:17.594653Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Créer des paires de tweets pour l'entraînement et les tests\ntrain_tweet_pairs = create_random_tweet_pairs(train_data, 100)\ntest_tweet_pairs = create_random_tweet_pairs(test_data, 50)\n\n# Étiqueter les paires de tweets pour l'entraînement et les tests\nlabeled_train_pairs = label_tweet_pairs(train_tweet_pairs, train_data)\nlabeled_test_pairs = label_tweet_pairs(test_tweet_pairs, test_data)\n\n# Convertir les paires étiquetées en DataFrame pour une manipulation facile\ntrain_df = pd.DataFrame(labeled_train_pairs, columns=['text1', 'text2', 'user1', 'user2', 'label'])\ntest_df = pd.DataFrame(labeled_test_pairs, columns=['text1', 'text2', 'user1', 'user2', 'label'])\n\n# Enregistrer les données étiquetées dans de nouveaux fichiers Excel\ntrain_df.to_excel('train_labeled.xlsx', index=False)\ntest_df.to_excel('test_labeled.xlsx', index=False)\n\n# Afficher un exemple de paire de tweets étiquetée\nprint(\"Exemple de paire de tweets étiquetée pour les données d'entraînement:\")\nprint(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:04:17.598052Z","iopub.execute_input":"2024-03-18T21:04:17.598482Z","iopub.status.idle":"2024-03-18T21:04:17.808138Z","shell.execute_reply.started":"2024-03-18T21:04:17.598446Z","shell.execute_reply":"2024-03-18T21:04:17.807180Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Exemple de paire de tweets étiquetée pour les données d'entraînement:\n                                               text1  \\\n0  We need your support to win the #PepsiRefresh ...   \n1  on my knees just praying for my Caribbean peop...   \n2  @joanneijoanna I said those r criminal thought...   \n3  @nofloodshooker ART IS THE OnLY ReAL class pay...   \n4  Swimming with Sharks is one of my favorite thi...   \n\n                                               text2     user1     user2  \\\n0  #ESQUIREuk #ukNavy #CoverStory http://instagra...   rihanna   rihanna   \n1  #glamour #ratedRera http://instagram.com/p/a8g...   rihanna   rihanna   \n2                          I'm in LONDON bitchesssss  ladygaga   rihanna   \n3  @BarackObama thanku for the support you are se...  ladygaga  ladygaga   \n4  He want dat... #CAKE RT @EvelynLozada: @rihann...   rihanna   rihanna   \n\n   label  \n0      1  \n1      1  \n2      0  \n3      1  \n4      1  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LA DEUXIEME PARTIE: data preprocessing","metadata":{}},{"cell_type":"code","source":"# Initialiser le tokenizer pour les tweets\ntokenizer = TweetTokenizer()\n# Initialiser le stemmer\nstemmer = PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:04:17.809308Z","iopub.execute_input":"2024-03-18T21:04:17.809600Z","iopub.status.idle":"2024-03-18T21:04:17.814175Z","shell.execute_reply.started":"2024-03-18T21:04:17.809574Z","shell.execute_reply":"2024-03-18T21:04:17.813107Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Fonction de nettoyage du texte\ndef clean_text(text):\n    # Convertir en minuscules\n    text = text.lower()\n    # Supprimer la ponctuation, sauf les hashtags et mentions\n    text = ''.join([char for char in text if char not in string.punctuation or char in ['#', '@']])\n    # Tokenization\n    tokens = tokenizer.tokenize(text)\n    # Supprimer les mots vides (stop words)\n    tokens = [word for word in tokens if word not in stopwords.words('english')]\n    # Stemming\n    tokens = [stemmer.stem(word) for word in tokens]\n    # Reformer le texte à partir des tokens\n    cleaned_text = ' '.join(tokens)\n    return cleaned_text","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:04:17.815174Z","iopub.execute_input":"2024-03-18T21:04:17.815447Z","iopub.status.idle":"2024-03-18T21:04:17.824056Z","shell.execute_reply.started":"2024-03-18T21:04:17.815425Z","shell.execute_reply":"2024-03-18T21:04:17.823052Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Appliquer la fonction de nettoyage aux données d'entraînement\ntrain_data['cleaned_text'] = train_data['text'].apply(clean_text)\n# Appliquer la fonction de nettoyage aux données de test\ntest_data['cleaned_text'] = test_data['text'].apply(clean_text)\n\n# Afficher les données après nettoyage\nprint(\"Exemple de données d'entraînement après nettoyage:\")\nprint(train_data[['text', 'cleaned_text']].head())","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:04:17.825285Z","iopub.execute_input":"2024-03-18T21:04:17.825650Z","iopub.status.idle":"2024-03-18T21:04:18.687854Z","shell.execute_reply.started":"2024-03-18T21:04:17.825591Z","shell.execute_reply":"2024-03-18T21:04:18.686807Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Exemple de données d'entraînement après nettoyage:\n                                                text  \\\n0  @BarackObama thanku for the support you are se...   \n1  The first time Tom Ford and Nick Knight worked...   \n2  I feel absolutely fabulous.pic.twitter.com/NZC...   \n3  #BraveCharlie bornthiswayfoundation, an opport...   \n4  Chipmunk Cheeks   Wisdom Teeth out before tour...   \n\n                                        cleaned_text  \n0  @barackobama thanku support send mother time v...  \n1  first time tom ford nick knight work togeth go...  \n2        feel absolut fabulouspictwittercomnzcb 9zzn  \n3  #bravecharli bornthiswayfound opportun peopl w...  \n4  chipmunk cheek wisdom teeth tour cant eat im g...  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LA TROISIEME PARTIE:","metadata":{}},{"cell_type":"code","source":"# Embedding Layer\n# Download and load the pre-trained Word2Vec embeddings using torchtext\nword2vec = vocab.GloVe(name='6B', dim=300)  # You can specify other dimensions if needed\n\n\n# Function to get the embeddings of each word in a text\ndef get_text_embeddings(text, model):\n    tokens = text.split()\n    embeddings = []\n    for token in tokens:\n        if token in model.stoi:\n            embeddings.append(model.vectors[model.stoi[token]].tolist())  # Convert tensor to list\n    if len(embeddings) == 0:\n        # If no word has an embedding, return a list of zeros\n        return [[0.0] * model.vectors.size(1)]\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:04:18.688913Z","iopub.execute_input":"2024-03-18T21:04:18.689184Z","iopub.status.idle":"2024-03-18T21:08:24.656439Z","shell.execute_reply.started":"2024-03-18T21:04:18.689160Z","shell.execute_reply":"2024-03-18T21:08:24.655462Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":".vector_cache/glove.6B.zip: 862MB [02:41, 5.35MB/s]                               \n100%|█████████▉| 399999/400000 [01:07<00:00, 5884.06it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Apply the function to get the embeddings on the training and test data\ntrain_data['text_embeddings'] = train_data['cleaned_text'].apply(lambda x: get_text_embeddings(x, word2vec))\ntest_data['text_embeddings'] = test_data['cleaned_text'].apply(lambda x: get_text_embeddings(x, word2vec))\n\n# Save the data with embeddings to new Excel files\ntrain_data.to_excel('train_embeddings.xlsx', index=False)\ntest_data.to_excel('test_embeddings.xlsx', index=False)\n\n# Example of displaying the data after adding the embeddings\nprint(\"Example of training data with embeddings:\")\nprint(train_data[['cleaned_text', 'text_embeddings']].head())","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:08:24.657747Z","iopub.execute_input":"2024-03-18T21:08:24.658039Z","iopub.status.idle":"2024-03-18T21:08:26.777377Z","shell.execute_reply.started":"2024-03-18T21:08:24.658015Z","shell.execute_reply":"2024-03-18T21:08:26.776395Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Example of training data with embeddings:\n                                        cleaned_text  \\\n0  @barackobama thanku support send mother time v...   \n1  first time tom ford nick knight work togeth go...   \n2        feel absolut fabulouspictwittercomnzcb 9zzn   \n3  #bravecharli bornthiswayfound opportun peopl w...   \n4  chipmunk cheek wisdom teeth tour cant eat im g...   \n\n                                     text_embeddings  \n0  [[-0.28084999322891235, -0.23964999616146088, ...  \n1  [[-0.15560999512672424, 0.5006899833679199, -0...  \n2  [[0.19787000119686127, 0.10199999809265137, 0....  \n3  [[-0.2583099901676178, 0.43643999099731445, -0...  \n4  [[0.4510299861431122, 0.20100000500679016, 0.1...  \n","output_type":"stream"}]},{"cell_type":"code","source":"#partiiiie 3.2\n# Load pre-trained transformer model and tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n\n\n# Function to get embeddings using transformer encoder\ndef get_transformer_embeddings(text, model, tokenizer):\n    input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n    with torch.no_grad():\n        outputs = model(input_ids)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Convert tensor to list\n    return embeddings\n\n\n# Apply the function to get embeddings on the training and test data\ntrain_data['text_embeddings'] = train_data['cleaned_text'].apply(\n    lambda x: get_transformer_embeddings(x, model, tokenizer))\ntest_data['text_embeddings'] = test_data['cleaned_text'].apply(\n    lambda x: get_transformer_embeddings(x, model, tokenizer))\n\n# Save the data with embeddings to new Excel files\ntrain_data.to_excel('train_transformer_embeddings.xlsx', index=False)\ntest_data.to_excel('test_transformer_embeddings.xlsx', index=False)\n\nprint(\"Exemple de données d'entraînement avec embeddings BERT :\")\nprint(train_data[['cleaned_text', 'text_embeddings']].head())","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:08:26.780821Z","iopub.execute_input":"2024-03-18T21:08:26.781168Z","iopub.status.idle":"2024-03-18T21:08:58.842543Z","shell.execute_reply.started":"2024-03-18T21:08:26.781142Z","shell.execute_reply":"2024-03-18T21:08:58.841498Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ec0bfacc7d46168e4f897fde74d570"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f7f6e7a339e448f89bca3d609881044"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e506bcfb2e8457fbd5f193773516f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1207cef22974cd8b59157d0fd16d16f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa148d32c6874a3480b5caf31455c84a"}},"metadata":{}},{"name":"stdout","text":"Exemple de données d'entraînement avec embeddings BERT :\n                                        cleaned_text  \\\n0  @barackobama thanku support send mother time v...   \n1  first time tom ford nick knight work togeth go...   \n2        feel absolut fabulouspictwittercomnzcb 9zzn   \n3  #bravecharli bornthiswayfound opportun peopl w...   \n4  chipmunk cheek wisdom teeth tour cant eat im g...   \n\n                                     text_embeddings  \n0  [0.1966565102338791, -0.12563742697238922, 0.6...  \n1  [-0.0633881464600563, 0.1929583102464676, 0.59...  \n2  [0.01856006495654583, 0.018714340403676033, 1....  \n3  [0.24664439260959625, 0.09449848532676697, 0.6...  \n4  [0.2095598727464676, 0.1293947845697403, 0.575...  \n","output_type":"stream"}]},{"cell_type":"code","source":"#partieeeee3.3\n# Feature Extraction\n# Define a function to extract features from the transformer encoder output\ndef extract_features(tweet_embeddings):\n    # Convert list of embeddings to tensor\n    tweet_embeddings_tensor = torch.tensor(tweet_embeddings)\n    # Convert the tensor to float type\n    tweet_embeddings_tensor = tweet_embeddings_tensor.float()\n    # Return the tweet embeddings tensor\n    return tweet_embeddings_tensor\n\n\n# Apply the function to extract features on the training and test data\ntrain_data['features'] = train_data['text_embeddings'].apply(extract_features)\ntest_data['features'] = test_data['text_embeddings'].apply(extract_features)\n\n# Example of displaying the data after feature extraction\nprint(\"Example of training data with extracted features:\")\nprint(train_data[['cleaned_text', 'features']].head())","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:08:58.844285Z","iopub.execute_input":"2024-03-18T21:08:58.844568Z","iopub.status.idle":"2024-03-18T21:08:59.079647Z","shell.execute_reply.started":"2024-03-18T21:08:58.844543Z","shell.execute_reply":"2024-03-18T21:08:59.078683Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Example of training data with extracted features:\n                                        cleaned_text  \\\n0  @barackobama thanku support send mother time v...   \n1  first time tom ford nick knight work togeth go...   \n2        feel absolut fabulouspictwittercomnzcb 9zzn   \n3  #bravecharli bornthiswayfound opportun peopl w...   \n4  chipmunk cheek wisdom teeth tour cant eat im g...   \n\n                                            features  \n0  [tensor(0.1967), tensor(-0.1256), tensor(0.690...  \n1  [tensor(-0.0634), tensor(0.1930), tensor(0.597...  \n2  [tensor(0.0186), tensor(0.0187), tensor(1.0780...  \n3  [tensor(0.2466), tensor(0.0945), tensor(0.6304...  \n4  [tensor(0.2096), tensor(0.1294), tensor(0.5754...  \n","output_type":"stream"}]},{"cell_type":"code","source":"#partieeeee3.4\n# Manhattan Distance Calculation\n# Define a function to calculate the Manhattan distance between two tweet representations\ndef calculate_manhattan_distance(features1, features2):\n    # Ensure both features have the same shape\n    if features1.shape != features2.shape:\n        # Broadcast features to a common shape\n        max_shape = torch.tensor([max(s1, s2) for s1, s2 in zip(features1.shape, features2.shape)])\n        features1 = features1.expand(max_shape)\n        features2 = features2.expand(max_shape)\n\n    # Calculate the absolute difference between the features\n    absolute_difference = torch.abs(features1 - features2)\n\n    # Sum the absolute differences along the appropriate dimensions\n    if len(absolute_difference.shape) > 1:\n        # If the tensor has more than one dimension, sum along axis 1\n        manhattan_distance = torch.sum(absolute_difference, dim=1)\n    else:\n        # If the tensor has only one dimension, sum directly\n        manhattan_distance = absolute_difference\n\n    # Return the Manhattan distance\n    return manhattan_distance\n\n\n# Apply the function to calculate Manhattan distance on the training and test data\ntrain_data['manhattan_distance'] = train_data.apply(\n    lambda row: calculate_manhattan_distance(row['features'][0], row['features'][1]), axis=1)\ntest_data['manhattan_distance'] = test_data.apply(\n    lambda row: calculate_manhattan_distance(row['features'][0], row['features'][1]), axis=1)\n\n# Example of displaying the data after Manhattan distance calculation\nprint(\"Example of training data with Manhattan distance:\")\nprint(train_data[['cleaned_text', 'manhattan_distance']].head())","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:08:59.080741Z","iopub.execute_input":"2024-03-18T21:08:59.081027Z","iopub.status.idle":"2024-03-18T21:08:59.115821Z","shell.execute_reply.started":"2024-03-18T21:08:59.081002Z","shell.execute_reply":"2024-03-18T21:08:59.114993Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Example of training data with Manhattan distance:\n                                        cleaned_text manhattan_distance\n0  @barackobama thanku support send mother time v...     tensor(0.3223)\n1  first time tom ford nick knight work togeth go...     tensor(0.2563)\n2        feel absolut fabulouspictwittercomnzcb 9zzn     tensor(0.0002)\n3  #bravecharli bornthiswayfound opportun peopl w...     tensor(0.1521)\n4  chipmunk cheek wisdom teeth tour cant eat im g...     tensor(0.0802)\n","output_type":"stream"}]},{"cell_type":"code","source":"#partieeeeee3.5\n# Define a function to add a dense layer with sigmoid activation\nclass DenseLayer(nn.Module):\n    def __init__(self, input_dim):\n        super(DenseLayer, self).__init__()\n        self.dense = nn.Linear(input_dim, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.dense(x)\n        x = self.sigmoid(x)\n        return x\n\n# Apply the function to calculate Manhattan distance on the training and test data\ntrain_data['manhattan_distance'] = train_data.apply(\n    lambda row: calculate_manhattan_distance(row['features'][0], row['features'][1]), axis=1)\ntest_data['manhattan_distance'] = test_data.apply(\n    lambda row: calculate_manhattan_distance(row['features'][0], row['features'][1]), axis=1)\n\n# Calculate the input dimension for the dense layer\ninput_dim = train_data['manhattan_distance'].apply(lambda x: x.shape[0] if len(x.shape) > 0 else 1).iloc[0]\n\n# Check if the input dimension is valid\nif input_dim > 0:\n    # Initialize the dense layer\n    dense_layer = DenseLayer(input_dim)\n\n    # Apply the dense layer to get the similarity score on the training and test data\n    train_data['similarity_score'] = train_data['manhattan_distance'].apply(lambda x: dense_layer(x) if len(x.shape) > 0 else dense_layer(torch.zeros(1)))\n    test_data['similarity_score'] = test_data['manhattan_distance'].apply(lambda x: dense_layer(x) if len(x.shape) > 0 else dense_layer(torch.zeros(1)))\n\n    # Example of displaying the data after adding similarity scores\n    print(\"Example of training data with similarity scores:\")\n    print(train_data[['cleaned_text', 'similarity_score']].head())\nelse:\n    print(\"No valid input dimension found. Check the 'manhattan_distance' column.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:08:59.116825Z","iopub.execute_input":"2024-03-18T21:08:59.117089Z","iopub.status.idle":"2024-03-18T21:08:59.176677Z","shell.execute_reply.started":"2024-03-18T21:08:59.117067Z","shell.execute_reply":"2024-03-18T21:08:59.175796Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Example of training data with similarity scores:\n                                        cleaned_text  \\\n0  @barackobama thanku support send mother time v...   \n1  first time tom ford nick knight work togeth go...   \n2        feel absolut fabulouspictwittercomnzcb 9zzn   \n3  #bravecharli bornthiswayfound opportun peopl w...   \n4  chipmunk cheek wisdom teeth tour cant eat im g...   \n\n                              similarity_score  \n0  [tensor(0.6356, grad_fn=<UnbindBackward0>)]  \n1  [tensor(0.6356, grad_fn=<UnbindBackward0>)]  \n2  [tensor(0.6356, grad_fn=<UnbindBackward0>)]  \n3  [tensor(0.6356, grad_fn=<UnbindBackward0>)]  \n4  [tensor(0.6356, grad_fn=<UnbindBackward0>)]  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LA QUATRIEME PARTIE: evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate(tweet_pairs, true_labels):\n    true_positives = sum(1 for pair, label in zip(tweet_pairs, true_labels) if label == 1)\n    false_positives = sum(1 for pair, label in zip(tweet_pairs, true_labels) if label == 0)\n    false_negatives = sum(1 for pair, label in zip(tweet_pairs, true_labels) if label == 1)\n\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n\n    return precision, recall, f1_score","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:08:59.177681Z","iopub.execute_input":"2024-03-18T21:08:59.177939Z","iopub.status.idle":"2024-03-18T21:08:59.185065Z","shell.execute_reply.started":"2024-03-18T21:08:59.177917Z","shell.execute_reply":"2024-03-18T21:08:59.184136Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"true_labels = [pair[4] for pair in labeled_test_pairs]  # Extracting true labels from labeled_test_pairs\nprecision, recall, f1_score = evaluate(labeled_test_pairs, true_labels)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1_score)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T21:08:59.186175Z","iopub.execute_input":"2024-03-18T21:08:59.186496Z","iopub.status.idle":"2024-03-18T21:08:59.199449Z","shell.execute_reply.started":"2024-03-18T21:08:59.186465Z","shell.execute_reply":"2024-03-18T21:08:59.198647Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Precision: 0.6\nRecall: 0.5\nF1 Score: 0.5454545454545454\n","output_type":"stream"}]}]}