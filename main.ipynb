{"cells":[{"cell_type":"markdown","metadata":{},"source":["# IMPORT PACKAGES"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:04:07.264312Z","iopub.status.busy":"2024-03-18T21:04:07.264032Z","iopub.status.idle":"2024-03-18T21:04:16.971751Z","shell.execute_reply":"2024-03-18T21:04:16.970846Z","shell.execute_reply.started":"2024-03-18T21:04:07.264286Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import random\n","from transformers import BertTokenizer, BertModel\n","import torch\n","import string\n","import nltk\n","from nltk.tokenize import TweetTokenizer\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import torchtext.vocab as vocab\n","from torch import nn"]},{"cell_type":"markdown","metadata":{},"source":["# LA PREMIERE PARTIE: data preparation"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:04:16.974016Z","iopub.status.busy":"2024-03-18T21:04:16.973464Z","iopub.status.idle":"2024-03-18T21:04:17.585656Z","shell.execute_reply":"2024-03-18T21:04:17.584953Z","shell.execute_reply.started":"2024-03-18T21:04:16.973983Z"},"trusted":true},"outputs":[],"source":["# Load the dataset\n","train_data = pd.read_excel('/kaggle/input/tweets-datasetsxlsx/test.xlsx', nrows=200)\n","test_data = pd.read_excel('/kaggle/input/tweets-datasetsxlsx/test.xlsx', nrows=200)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:04:17.587273Z","iopub.status.busy":"2024-03-18T21:04:17.586794Z","iopub.status.idle":"2024-03-18T21:04:17.595588Z","shell.execute_reply":"2024-03-18T21:04:17.594653Z","shell.execute_reply.started":"2024-03-18T21:04:17.587244Z"},"trusted":true},"outputs":[],"source":["# Fonction pour créer des paires de tweets aléatoires\n","def create_random_tweet_pairs(data, num_pairs):\n","    tweet_pairs = []\n","    tweet_indices = list(range(len(data)))\n","    for _ in range(num_pairs):\n","        i, j = random.sample(tweet_indices, 2)\n","        tweet_pairs.append((data.iloc[i]['text'], data.iloc[j]['text']))\n","    return tweet_pairs\n","\n","\n","# Fonction pour étiqueter les paires de tweets en fonction de la similarité des utilisateurs\n","def label_tweet_pairs(tweet_pairs, data):\n","    labeled_pairs = []\n","    for pair in tweet_pairs:\n","        tweet1, tweet2 = pair\n","        user1 = data[data['text'] == tweet1]['user'].iloc[0]\n","        user2 = data[data['text'] == tweet2]['user'].iloc[0]\n","        similarity_label = 1 if user1 == user2 else 0\n","        labeled_pairs.append((tweet1, tweet2, user1, user2, similarity_label))\n","    return labeled_pairs"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:04:17.598482Z","iopub.status.busy":"2024-03-18T21:04:17.598052Z","iopub.status.idle":"2024-03-18T21:04:17.808138Z","shell.execute_reply":"2024-03-18T21:04:17.807180Z","shell.execute_reply.started":"2024-03-18T21:04:17.598446Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Exemple de paire de tweets étiquetée pour les données d'entraînement:\n","                                               text1  \\\n","0  We need your support to win the #PepsiRefresh ...   \n","1  on my knees just praying for my Caribbean peop...   \n","2  @joanneijoanna I said those r criminal thought...   \n","3  @nofloodshooker ART IS THE OnLY ReAL class pay...   \n","4  Swimming with Sharks is one of my favorite thi...   \n","\n","                                               text2     user1     user2  \\\n","0  #ESQUIREuk #ukNavy #CoverStory http://instagra...   rihanna   rihanna   \n","1  #glamour #ratedRera http://instagram.com/p/a8g...   rihanna   rihanna   \n","2                          I'm in LONDON bitchesssss  ladygaga   rihanna   \n","3  @BarackObama thanku for the support you are se...  ladygaga  ladygaga   \n","4  He want dat... #CAKE RT @EvelynLozada: @rihann...   rihanna   rihanna   \n","\n","   label  \n","0      1  \n","1      1  \n","2      0  \n","3      1  \n","4      1  \n"]}],"source":["# Créer des paires de tweets pour l'entraînement et les tests\n","train_tweet_pairs = create_random_tweet_pairs(train_data, 100)\n","test_tweet_pairs = create_random_tweet_pairs(test_data, 50)\n","\n","# Étiqueter les paires de tweets pour l'entraînement et les tests\n","labeled_train_pairs = label_tweet_pairs(train_tweet_pairs, train_data)\n","labeled_test_pairs = label_tweet_pairs(test_tweet_pairs, test_data)\n","\n","# Convertir les paires étiquetées en DataFrame pour une manipulation facile\n","train_df = pd.DataFrame(labeled_train_pairs, columns=['text1', 'text2', 'user1', 'user2', 'label'])\n","test_df = pd.DataFrame(labeled_test_pairs, columns=['text1', 'text2', 'user1', 'user2', 'label'])\n","\n","# Enregistrer les données étiquetées dans de nouveaux fichiers Excel\n","train_df.to_excel('train_labeled.xlsx', index=False)\n","test_df.to_excel('test_labeled.xlsx', index=False)\n","\n","# Afficher un exemple de paire de tweets étiquetée\n","print(\"Exemple de paire de tweets étiquetée pour les données d'entraînement:\")\n","print(train_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["# LA DEUXIEME PARTIE: data preprocessing"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:04:17.809600Z","iopub.status.busy":"2024-03-18T21:04:17.809308Z","iopub.status.idle":"2024-03-18T21:04:17.814175Z","shell.execute_reply":"2024-03-18T21:04:17.813107Z","shell.execute_reply.started":"2024-03-18T21:04:17.809574Z"},"trusted":true},"outputs":[],"source":["# Initialiser le tokenizer pour les tweets\n","tokenizer = TweetTokenizer()\n","# Initialiser le stemmer\n","stemmer = PorterStemmer()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:04:17.815447Z","iopub.status.busy":"2024-03-18T21:04:17.815174Z","iopub.status.idle":"2024-03-18T21:04:17.824056Z","shell.execute_reply":"2024-03-18T21:04:17.823052Z","shell.execute_reply.started":"2024-03-18T21:04:17.815425Z"},"trusted":true},"outputs":[],"source":["# Fonction de nettoyage du texte\n","def clean_text(text):\n","    # Convertir en minuscules\n","    text = text.lower()\n","    # Supprimer la ponctuation, sauf les hashtags et mentions\n","    text = ''.join([char for char in text if char not in string.punctuation or char in ['#', '@']])\n","    # Tokenization\n","    tokens = tokenizer.tokenize(text)\n","    # Supprimer les mots vides (stop words)\n","    tokens = [word for word in tokens if word not in stopwords.words('english')]\n","    # Stemming\n","    tokens = [stemmer.stem(word) for word in tokens]\n","    # Reformer le texte à partir des tokens\n","    cleaned_text = ' '.join(tokens)\n","    return cleaned_text"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:04:17.825650Z","iopub.status.busy":"2024-03-18T21:04:17.825285Z","iopub.status.idle":"2024-03-18T21:04:18.687854Z","shell.execute_reply":"2024-03-18T21:04:18.686807Z","shell.execute_reply.started":"2024-03-18T21:04:17.825591Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Exemple de données d'entraînement après nettoyage:\n","                                                text  \\\n","0  @BarackObama thanku for the support you are se...   \n","1  The first time Tom Ford and Nick Knight worked...   \n","2  I feel absolutely fabulous.pic.twitter.com/NZC...   \n","3  #BraveCharlie bornthiswayfoundation, an opport...   \n","4  Chipmunk Cheeks   Wisdom Teeth out before tour...   \n","\n","                                        cleaned_text  \n","0  @barackobama thanku support send mother time v...  \n","1  first time tom ford nick knight work togeth go...  \n","2        feel absolut fabulouspictwittercomnzcb 9zzn  \n","3  #bravecharli bornthiswayfound opportun peopl w...  \n","4  chipmunk cheek wisdom teeth tour cant eat im g...  \n"]}],"source":["# Appliquer la fonction de nettoyage aux données d'entraînement\n","train_data['cleaned_text'] = train_data['text'].apply(clean_text)\n","# Appliquer la fonction de nettoyage aux données de test\n","test_data['cleaned_text'] = test_data['text'].apply(clean_text)\n","\n","# Afficher les données après nettoyage\n","print(\"Exemple de données d'entraînement après nettoyage:\")\n","print(train_data[['text', 'cleaned_text']].head())"]},{"cell_type":"markdown","metadata":{},"source":["# LA TROISIEME PARTIE:"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:04:18.689184Z","iopub.status.busy":"2024-03-18T21:04:18.688913Z","iopub.status.idle":"2024-03-18T21:08:24.656439Z","shell.execute_reply":"2024-03-18T21:08:24.655462Z","shell.execute_reply.started":"2024-03-18T21:04:18.689160Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [02:41, 5.35MB/s]                               \n","100%|█████████▉| 399999/400000 [01:07<00:00, 5884.06it/s]\n"]}],"source":["# Embedding Layer\n","# Download and load the pre-trained Word2Vec embeddings using torchtext\n","word2vec = vocab.GloVe(name='6B', dim=300)  # You can specify other dimensions if needed\n","\n","\n","# Function to get the embeddings of each word in a text\n","def get_text_embeddings(text, model):\n","    tokens = text.split()\n","    embeddings = []\n","    for token in tokens:\n","        if token in model.stoi:\n","            embeddings.append(model.vectors[model.stoi[token]].tolist())  # Convert tensor to list\n","    if len(embeddings) == 0:\n","        # If no word has an embedding, return a list of zeros\n","        return [[0.0] * model.vectors.size(1)]\n","    return embeddings"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:08:24.658039Z","iopub.status.busy":"2024-03-18T21:08:24.657747Z","iopub.status.idle":"2024-03-18T21:08:26.777377Z","shell.execute_reply":"2024-03-18T21:08:26.776395Z","shell.execute_reply.started":"2024-03-18T21:08:24.658015Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Example of training data with embeddings:\n","                                        cleaned_text  \\\n","0  @barackobama thanku support send mother time v...   \n","1  first time tom ford nick knight work togeth go...   \n","2        feel absolut fabulouspictwittercomnzcb 9zzn   \n","3  #bravecharli bornthiswayfound opportun peopl w...   \n","4  chipmunk cheek wisdom teeth tour cant eat im g...   \n","\n","                                     text_embeddings  \n","0  [[-0.28084999322891235, -0.23964999616146088, ...  \n","1  [[-0.15560999512672424, 0.5006899833679199, -0...  \n","2  [[0.19787000119686127, 0.10199999809265137, 0....  \n","3  [[-0.2583099901676178, 0.43643999099731445, -0...  \n","4  [[0.4510299861431122, 0.20100000500679016, 0.1...  \n"]}],"source":["# Apply the function to get the embeddings on the training and test data\n","train_data['text_embeddings'] = train_data['cleaned_text'].apply(lambda x: get_text_embeddings(x, word2vec))\n","test_data['text_embeddings'] = test_data['cleaned_text'].apply(lambda x: get_text_embeddings(x, word2vec))\n","\n","# Save the data with embeddings to new Excel files\n","train_data.to_excel('train_embeddings.xlsx', index=False)\n","test_data.to_excel('test_embeddings.xlsx', index=False)\n","\n","# Example of displaying the data after adding the embeddings\n","print(\"Example of training data with embeddings:\")\n","print(train_data[['cleaned_text', 'text_embeddings']].head())"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:08:26.781168Z","iopub.status.busy":"2024-03-18T21:08:26.780821Z","iopub.status.idle":"2024-03-18T21:08:58.842543Z","shell.execute_reply":"2024-03-18T21:08:58.841498Z","shell.execute_reply.started":"2024-03-18T21:08:26.781142Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78ec0bfacc7d46168e4f897fde74d570","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f7f6e7a339e448f89bca3d609881044","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e506bcfb2e8457fbd5f193773516f08","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1207cef22974cd8b59157d0fd16d16f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa148d32c6874a3480b5caf31455c84a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Exemple de données d'entraînement avec embeddings BERT :\n","                                        cleaned_text  \\\n","0  @barackobama thanku support send mother time v...   \n","1  first time tom ford nick knight work togeth go...   \n","2        feel absolut fabulouspictwittercomnzcb 9zzn   \n","3  #bravecharli bornthiswayfound opportun peopl w...   \n","4  chipmunk cheek wisdom teeth tour cant eat im g...   \n","\n","                                     text_embeddings  \n","0  [0.1966565102338791, -0.12563742697238922, 0.6...  \n","1  [-0.0633881464600563, 0.1929583102464676, 0.59...  \n","2  [0.01856006495654583, 0.018714340403676033, 1....  \n","3  [0.24664439260959625, 0.09449848532676697, 0.6...  \n","4  [0.2095598727464676, 0.1293947845697403, 0.575...  \n"]}],"source":["#partiiiie 3.2\n","# Load pre-trained transformer model and tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model = BertModel.from_pretrained(\"bert-base-uncased\")\n","\n","\n","# Function to get embeddings using transformer encoder\n","def get_transformer_embeddings(text, model, tokenizer):\n","    input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n","    with torch.no_grad():\n","        outputs = model(input_ids)\n","    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()  # Convert tensor to list\n","    return embeddings\n","\n","\n","# Apply the function to get embeddings on the training and test data\n","train_data['text_embeddings'] = train_data['cleaned_text'].apply(\n","    lambda x: get_transformer_embeddings(x, model, tokenizer))\n","test_data['text_embeddings'] = test_data['cleaned_text'].apply(\n","    lambda x: get_transformer_embeddings(x, model, tokenizer))\n","\n","# Save the data with embeddings to new Excel files\n","train_data.to_excel('train_transformer_embeddings.xlsx', index=False)\n","test_data.to_excel('test_transformer_embeddings.xlsx', index=False)\n","\n","print(\"Exemple de données d'entraînement avec embeddings BERT :\")\n","print(train_data[['cleaned_text', 'text_embeddings']].head())"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:08:58.844568Z","iopub.status.busy":"2024-03-18T21:08:58.844285Z","iopub.status.idle":"2024-03-18T21:08:59.079647Z","shell.execute_reply":"2024-03-18T21:08:59.078683Z","shell.execute_reply.started":"2024-03-18T21:08:58.844543Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Example of training data with extracted features:\n","                                        cleaned_text  \\\n","0  @barackobama thanku support send mother time v...   \n","1  first time tom ford nick knight work togeth go...   \n","2        feel absolut fabulouspictwittercomnzcb 9zzn   \n","3  #bravecharli bornthiswayfound opportun peopl w...   \n","4  chipmunk cheek wisdom teeth tour cant eat im g...   \n","\n","                                            features  \n","0  [tensor(0.1967), tensor(-0.1256), tensor(0.690...  \n","1  [tensor(-0.0634), tensor(0.1930), tensor(0.597...  \n","2  [tensor(0.0186), tensor(0.0187), tensor(1.0780...  \n","3  [tensor(0.2466), tensor(0.0945), tensor(0.6304...  \n","4  [tensor(0.2096), tensor(0.1294), tensor(0.5754...  \n"]}],"source":["#partieeeee3.3\n","# Feature Extraction\n","# Define a function to extract features from the transformer encoder output\n","def extract_features(tweet_embeddings):\n","    # Convert list of embeddings to tensor\n","    tweet_embeddings_tensor = torch.tensor(tweet_embeddings)\n","    # Convert the tensor to float type\n","    tweet_embeddings_tensor = tweet_embeddings_tensor.float()\n","    # Return the tweet embeddings tensor\n","    return tweet_embeddings_tensor\n","\n","\n","# Apply the function to extract features on the training and test data\n","train_data['features'] = train_data['text_embeddings'].apply(extract_features)\n","test_data['features'] = test_data['text_embeddings'].apply(extract_features)\n","\n","# Example of displaying the data after feature extraction\n","print(\"Example of training data with extracted features:\")\n","print(train_data[['cleaned_text', 'features']].head())"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:08:59.081027Z","iopub.status.busy":"2024-03-18T21:08:59.080741Z","iopub.status.idle":"2024-03-18T21:08:59.115821Z","shell.execute_reply":"2024-03-18T21:08:59.114993Z","shell.execute_reply.started":"2024-03-18T21:08:59.081002Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Example of training data with Manhattan distance:\n","                                        cleaned_text manhattan_distance\n","0  @barackobama thanku support send mother time v...     tensor(0.3223)\n","1  first time tom ford nick knight work togeth go...     tensor(0.2563)\n","2        feel absolut fabulouspictwittercomnzcb 9zzn     tensor(0.0002)\n","3  #bravecharli bornthiswayfound opportun peopl w...     tensor(0.1521)\n","4  chipmunk cheek wisdom teeth tour cant eat im g...     tensor(0.0802)\n"]}],"source":["#partieeeee3.4\n","# Manhattan Distance Calculation\n","# Define a function to calculate the Manhattan distance between two tweet representations\n","def calculate_manhattan_distance(features1, features2):\n","    # Ensure both features have the same shape\n","    if features1.shape != features2.shape:\n","        # Broadcast features to a common shape\n","        max_shape = torch.tensor([max(s1, s2) for s1, s2 in zip(features1.shape, features2.shape)])\n","        features1 = features1.expand(max_shape)\n","        features2 = features2.expand(max_shape)\n","\n","    # Calculate the absolute difference between the features\n","    absolute_difference = torch.abs(features1 - features2)\n","\n","    # Sum the absolute differences along the appropriate dimensions\n","    if len(absolute_difference.shape) > 1:\n","        # If the tensor has more than one dimension, sum along axis 1\n","        manhattan_distance = torch.sum(absolute_difference, dim=1)\n","    else:\n","        # If the tensor has only one dimension, sum directly\n","        manhattan_distance = absolute_difference\n","\n","    # Return the Manhattan distance\n","    return manhattan_distance\n","\n","\n","# Apply the function to calculate Manhattan distance on the training and test data\n","train_data['manhattan_distance'] = train_data.apply(\n","    lambda row: calculate_manhattan_distance(row['features'][0], row['features'][1]), axis=1)\n","test_data['manhattan_distance'] = test_data.apply(\n","    lambda row: calculate_manhattan_distance(row['features'][0], row['features'][1]), axis=1)\n","\n","# Example of displaying the data after Manhattan distance calculation\n","print(\"Example of training data with Manhattan distance:\")\n","print(train_data[['cleaned_text', 'manhattan_distance']].head())"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:08:59.117089Z","iopub.status.busy":"2024-03-18T21:08:59.116825Z","iopub.status.idle":"2024-03-18T21:08:59.176677Z","shell.execute_reply":"2024-03-18T21:08:59.175796Z","shell.execute_reply.started":"2024-03-18T21:08:59.117067Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Example of training data with similarity scores:\n","                                        cleaned_text  \\\n","0  @barackobama thanku support send mother time v...   \n","1  first time tom ford nick knight work togeth go...   \n","2        feel absolut fabulouspictwittercomnzcb 9zzn   \n","3  #bravecharli bornthiswayfound opportun peopl w...   \n","4  chipmunk cheek wisdom teeth tour cant eat im g...   \n","\n","                              similarity_score  \n","0  [tensor(0.6356, grad_fn=<UnbindBackward0>)]  \n","1  [tensor(0.6356, grad_fn=<UnbindBackward0>)]  \n","2  [tensor(0.6356, grad_fn=<UnbindBackward0>)]  \n","3  [tensor(0.6356, grad_fn=<UnbindBackward0>)]  \n","4  [tensor(0.6356, grad_fn=<UnbindBackward0>)]  \n"]}],"source":["#partieeeeee3.5\n","# Define a function to add a dense layer with sigmoid activation\n","class DenseLayer(nn.Module):\n","    def __init__(self, input_dim):\n","        super(DenseLayer, self).__init__()\n","        self.dense = nn.Linear(input_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","# Apply the function to calculate Manhattan distance on the training and test data\n","train_data['manhattan_distance'] = train_data.apply(\n","    lambda row: calculate_manhattan_distance(row['features'][0], row['features'][1]), axis=1)\n","test_data['manhattan_distance'] = test_data.apply(\n","    lambda row: calculate_manhattan_distance(row['features'][0], row['features'][1]), axis=1)\n","\n","# Calculate the input dimension for the dense layer\n","input_dim = train_data['manhattan_distance'].apply(lambda x: x.shape[0] if len(x.shape) > 0 else 1).iloc[0]\n","\n","# Check if the input dimension is valid\n","if input_dim > 0:\n","    # Initialize the dense layer\n","    dense_layer = DenseLayer(input_dim)\n","\n","    # Apply the dense layer to get the similarity score on the training and test data\n","    train_data['similarity_score'] = train_data['manhattan_distance'].apply(lambda x: dense_layer(x) if len(x.shape) > 0 else dense_layer(torch.zeros(1)))\n","    test_data['similarity_score'] = test_data['manhattan_distance'].apply(lambda x: dense_layer(x) if len(x.shape) > 0 else dense_layer(torch.zeros(1)))\n","\n","    # Example of displaying the data after adding similarity scores\n","    print(\"Example of training data with similarity scores:\")\n","    print(train_data[['cleaned_text', 'similarity_score']].head())\n","else:\n","    print(\"No valid input dimension found. Check the 'manhattan_distance' column.\")"]},{"cell_type":"markdown","metadata":{},"source":["# LA QUATRIEME PARTIE: evaluation"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:08:59.177939Z","iopub.status.busy":"2024-03-18T21:08:59.177681Z","iopub.status.idle":"2024-03-18T21:08:59.185065Z","shell.execute_reply":"2024-03-18T21:08:59.184136Z","shell.execute_reply.started":"2024-03-18T21:08:59.177917Z"},"trusted":true},"outputs":[],"source":["def evaluate(tweet_pairs, true_labels):\n","    true_positives = sum(1 for pair, label in zip(tweet_pairs, true_labels) if label == 1)\n","    false_positives = sum(1 for pair, label in zip(tweet_pairs, true_labels) if label == 0)\n","    false_negatives = sum(1 for pair, label in zip(tweet_pairs, true_labels) if label == 1)\n","\n","    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives != 0 else 0\n","    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives != 0 else 0\n","    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n","\n","    return precision, recall, f1_score"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-18T21:08:59.186496Z","iopub.status.busy":"2024-03-18T21:08:59.186175Z","iopub.status.idle":"2024-03-18T21:08:59.199449Z","shell.execute_reply":"2024-03-18T21:08:59.198647Z","shell.execute_reply.started":"2024-03-18T21:08:59.186465Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Precision: 0.6\n","Recall: 0.5\n","F1 Score: 0.5454545454545454\n"]}],"source":["true_labels = [pair[4] for pair in labeled_test_pairs]  # Extracting true labels from labeled_test_pairs\n","precision, recall, f1_score = evaluate(labeled_test_pairs, true_labels)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 Score:\", f1_score)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4574341,"sourceId":7810104,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
